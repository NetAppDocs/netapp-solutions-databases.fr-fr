---
sidebar: sidebar 
permalink: automation/automation-ora-asa-iscsi.html 
keywords: Database, Oracle, ASA, ONTAP, NetApp ASA 
summary: 'La solution fournit une vue d"ensemble et des détails pour le déploiement et la protection automatisés d"Oracle dans la baie NetApp ASA en tant que stockage de base de données principal avec le protocole iSCSI et la base de données Oracle configurée dans ReStart autonome à l"aide d"asm comme gestionnaire de volumes.' 
---
= TR-4983 : Déploiement Oracle simplifié et automatisé sur NetApp ASA avec iSCSI
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


Allen Cao, Niyaz Mohamed, NetApp

[role="lead"]
Cette solution fournit une vue d'ensemble et des détails pour le déploiement et la protection automatisés d'Oracle dans la baie NetApp ASA en tant que stockage de base de données principal avec le protocole iSCSI et la base de données Oracle configurée dans ReStart autonome à l'aide d'asm comme gestionnaire de volumes.



== But

Les systèmes NetApp ASA offrent des solutions modernes à votre infrastructure SAN.  Ils simplifient à grande échelle et vous permettent d'accélérer vos applications critiques telles que les bases de données, de garantir que vos données sont toujours disponibles (99,9999 % de disponibilité) et de réduire le coût total de possession et l'empreinte carbone.  Les systèmes NetApp ASA incluent des modèles de la série A conçus pour les applications les plus exigeantes en performances et des modèles de la série C optimisés pour des déploiements rentables et de grande capacité.  Ensemble, les systèmes ASA des séries A et C offrent des performances exceptionnelles pour améliorer l'expérience client et réduire le temps d'obtention des résultats, maintenir les données critiques pour l'entreprise disponibles, protégées et sécurisées, et fournir une capacité plus efficace pour n'importe quelle charge de travail, soutenue par la garantie la plus efficace du secteur.

Cette documentation démontre le déploiement simplifié des bases de données Oracle dans un environnement SAN construit avec des systèmes ASA à l'aide de l'automatisation Ansible.  La base de données Oracle est déployée dans une configuration ReStart autonome avec le protocole iSCSI pour l'accès aux données et Oracle ASM pour la gestion des disques de base de données sur la baie de stockage ASA .  Il fournit également des informations sur la sauvegarde, la restauration et le clonage de bases de données Oracle à l'aide de l'outil d'interface utilisateur NetApp SnapCenter pour un fonctionnement de base de données efficace en termes de stockage dans les systèmes NetApp ASA .

Cette solution répond aux cas d’utilisation suivants :

* Déploiement automatisé de bases de données Oracle dans les systèmes NetApp ASA comme stockage de base de données principal
* Sauvegarde et restauration de bases de données Oracle dans les systèmes NetApp ASA à l'aide de l'outil NetApp SnapCenter
* Clonage de base de données Oracle pour le développement/test ou d'autres cas d'utilisation dans les systèmes NetApp ASA à l'aide de l'outil NetApp SnapCenter




== Public

Cette solution est destinée aux personnes suivantes :

* Un administrateur de base de données qui souhaite déployer Oracle dans des systèmes NetApp ASA .
* Un architecte de solutions de base de données qui souhaite tester les charges de travail Oracle dans les systèmes NetApp ASA .
* Un administrateur de stockage souhaitant déployer et gérer une base de données Oracle sur des systèmes NetApp ASA .
* Un propriétaire d’application qui souhaite mettre en place une base de données Oracle dans les systèmes NetApp ASA .




== Environnement de test et de validation de solutions

Les tests et la validation de cette solution ont été réalisés dans un environnement de laboratoire qui pourrait ne pas correspondre à l’environnement de déploiement final.  Voir la section<<Facteurs clés à prendre en compte lors du déploiement>> pour plus d'informations.



=== Architecture

image:automation-ora-asa-iscsi-architecture.png["Cette image fournit une image détaillée de la configuration de déploiement Oracle dans le système NetApp ASA avec iSCSI et ASM."]



=== Composants matériels et logiciels

[cols="33%, 33%, 33%"]
|===


3+| *Matériel* 


| NetApp ASA A400 | Version 9.13.1P1 | 2 étagères NS224, 48 disques NVMe AFF avec une capacité totale de 69,3 Tio 


| UCSB-B200-M4 | Processeur Intel(R) Xeon(R) E5-2690 v4 à 2,60 GHz | Cluster VMware ESXi à 4 nœuds 


3+| *Logiciel* 


| RedHat Linux | Noyau RHEL-8.6, 4.18.0-372.9.1.el8.x86_64 | Abonnement RedHat déployé pour les tests 


| Windows Server | Norme 2022, 10.0.20348 Build 20348 | Hébergement du serveur SnapCenter 


| Infrastructure Oracle Grid | Version 19.18 | Patch RU appliqué p34762026_190000_Linux-x86-64.zip 


| Base de données Oracle | Version 19.18 | Patch RU appliqué p34765931_190000_Linux-x86-64.zip 


| Oracle OPatch | Version 12.2.0.1.36 | Dernier correctif p6880880_190000_Linux-x86-64.zip 


| Serveur SnapCenter | Version 4.9P1 | Déploiement de groupe de travail 


| Hyperviseur VMware vSphere | version 6.5.0.20000 | VMware Tools, versions : 11365 (Linux), 12352 (Windows) 


| Ouvrir le JDK | Version java-1.8.0-openjdk.x86_64 | Exigence du plug-in SnapCenter sur les machines virtuelles de base de données 
|===


=== Configuration de la base de données Oracle dans l'environnement de laboratoire

[cols="33%, 33%, 33%"]
|===


3+|  


| *Serveur* | *Base de données* | *Stockage de base de données* 


| ora_01 | NTAP1(NTAP1_PDB1,NTAP1_PDB2,NTAP1_PDB3) | LUN iSCSI sur ASA A400 


| ora_02 | NTAP2(NTAP2_PDB1,NTAP2_PDB2,NTAP2_PDB3) | LUN iSCSI sur ASA A400 
|===


=== Facteurs clés à prendre en compte lors du déploiement

* *Disposition de stockage de la base de données Oracle.*  Dans ce déploiement Oracle automatisé, nous provisionnons quatre volumes de base de données pour héberger les binaires, les données et les journaux Oracle par défaut.  Nous créons ensuite deux groupes de disques ASM à partir des données et des journaux LUN.  Au sein du groupe de disques asm +DATA, nous provisionnons deux LUN de données dans un volume sur chaque nœud de cluster ASA A400 .  Dans le groupe de disques asm +LOGS, nous créons deux LUN dans un volume de journal sur un seul nœud ASA A400 .  Plusieurs LUN disposés dans un volume ONTAP offrent de meilleures performances en général.
* *Déploiement de plusieurs serveurs de base de données.*  La solution d'automatisation peut déployer une base de données de conteneur Oracle sur plusieurs serveurs de base de données dans une seule exécution de playbook Ansible.  Quel que soit le nombre de serveurs de base de données, l’exécution du playbook reste la même.  En cas de déploiement de serveurs multi-DB, le playbook s'appuie sur un algorithme permettant de placer de manière optimale les LUN de base de données sur deux contrôleurs ASA A400 .  Les binaires et les journaux LUN du serveur DB à nombre impair dans l'index des hôtes du serveur sont placés sur le contrôleur 1.  Les binaires et les journaux LUN du serveur DB à numéro pair dans l'index des hôtes du serveur sont placés sur le contrôleur 2.  Les données de la base de données sont réparties uniformément sur deux contrôleurs.  Oracle ASM combine les LUN de données sur deux contrôleurs dans un seul groupe de disques ASM pour utiliser pleinement la puissance de traitement des deux contrôleurs.
* *Configuration iSCSI.*  Les machines virtuelles de base de données se connectent au stockage ASA avec le protocole iSCSI pour l'accès au stockage.  Vous devez configurer des chemins doubles sur chaque nœud de contrôleur pour la redondance et configurer le multi-chemin iSCSI sur le serveur de base de données pour l'accès au stockage multi-chemin.  Activez la trame jumbo sur le réseau de stockage pour maximiser les performances et le débit.
* *Niveau de redondance Oracle ASM à utiliser pour chaque groupe de disques Oracle ASM que vous créez.*  Étant donné que l' ASA A400 configure le stockage en RAID DP pour la protection des données au niveau du disque du cluster, vous devez utiliser `External Redundancy` , ce qui signifie que l'option ne permet pas à Oracle ASM de refléter le contenu du groupe de disques.
* *Sauvegarde de la base de données.*  NetApp fournit une suite SnapCenter software pour la sauvegarde, la restauration et le clonage de bases de données avec une interface utilisateur conviviale.  NetApp recommande de mettre en œuvre un tel outil de gestion pour réaliser une sauvegarde SnapShot rapide (moins d'une minute), une restauration rapide (quelques minutes) de la base de données et un clonage de la base de données.




== Déploiement de la solution

Les sections suivantes fournissent des procédures étape par étape pour le déploiement et la protection automatisés d'Oracle 19c dans NetApp ASA A400 avec des LUN de base de données montés directement via iSCSI vers la machine virtuelle DB dans un seul nœud. Redémarrez la configuration avec Oracle ASM comme gestionnaire de volume de base de données.



=== Prérequis pour le déploiement

[%collapsible%open]
====
Le déploiement nécessite les prérequis suivants.

. Il est supposé que la baie de stockage NetApp ASA a été installée et configurée.  Cela inclut le domaine de diffusion iSCSI, les groupes d'interfaces LACP a0a sur les deux nœuds de contrôleur, les ports VLAN iSCSI (a0a-<iscsi-a-vlan-id>, a0a-<iscsi-b-vlan-id>) sur les deux nœuds de contrôleur.  Le lien suivant fournit des instructions détaillées étape par étape si une aide est nécessaire.link:https://docs.netapp.com/us-en/ontap-systems/asa400/install-detailed-guide.html["Guide détaillé - ASA A400"^]
. Provisionnez une machine virtuelle Linux en tant que nœud de contrôleur Ansible avec la dernière version d'Ansible et de Git installée.  Consultez le lien suivant pour plus de détails :link:https://docs.netapp.com/us-en/netapp-solutions-dataops/automation/getting-started.html["Premiers pas avec l'automatisation des solutions NetApp ^"^] dans la section - `Setup the Ansible Control Node for CLI deployments on RHEL / CentOS` ou `Setup the Ansible Control Node for CLI deployments on Ubuntu / Debian` .
. Clonez une copie de la boîte à outils d’automatisation du déploiement NetApp Oracle pour iSCSI.
+
[source, cli]
----
git clone https://bitbucket.ngage.netapp.com/scm/ns-bb/na_oracle_deploy_iscsi.git
----
. Provisionnez un serveur Windows pour exécuter l’outil d’interface utilisateur NetApp SnapCenter avec la dernière version.  Consultez le lien suivant pour plus de détails :link:https://docs.netapp.com/us-en/snapcenter/install/task_install_the_snapcenter_server_using_the_install_wizard.html["Installer le serveur SnapCenter"^]
. Créez deux serveurs RHEL Oracle DB, soit bare metal, soit une machine virtuelle virtualisée.  Créez un utilisateur administrateur sur les serveurs de base de données avec sudo sans privilège de mot de passe et activez l'authentification par clé privée/publique SSH entre l'hôte Ansible et les hôtes du serveur de base de données Oracle.  Étape suivant l'installation des fichiers Oracle 19c sur le répertoire /tmp/archive des serveurs de base de données.
+
....
installer_archives:
  - "LINUX.X64_193000_grid_home.zip"
  - "p34762026_190000_Linux-x86-64.zip"
  - "LINUX.X64_193000_db_home.zip"
  - "p34765931_190000_Linux-x86-64.zip"
  - "p6880880_190000_Linux-x86-64.zip"
....
+

NOTE: Assurez-vous d'avoir alloué au moins 50 Go dans le volume racine d'Oracle VM pour disposer de suffisamment d'espace pour préparer les fichiers d'installation d'Oracle.

. Regardez la vidéo suivante :
+
.Déploiement Oracle simplifié et automatisé sur NetApp ASA avec iSCSI
video::79095731-6b02-41d5-9fa1-b0c00100d055[panopto,width=360]


====


=== Fichiers de paramètres d'automatisation

[%collapsible%open]
====
Le playbook Ansible exécute les tâches d'installation et de configuration de la base de données avec des paramètres prédéfinis.  Pour cette solution d’automatisation Oracle, il existe trois fichiers de paramètres définis par l’utilisateur qui nécessitent une saisie de l’utilisateur avant l’exécution du playbook.

* hôtes - définissez les cibles sur lesquelles le playbook d'automatisation s'exécute.
* vars/vars.yml - le fichier de variables globales qui définit les variables qui s'appliquent à toutes les cibles.
* host_vars/host_name.yml - le fichier de variables locales qui définit les variables qui s'appliquent uniquement à une cible locale.  Dans notre cas d’utilisation, il s’agit des serveurs de base de données Oracle.


En plus de ces fichiers de variables définis par l'utilisateur, il existe plusieurs fichiers de variables par défaut qui contiennent des paramètres par défaut qui ne nécessitent aucune modification, sauf si nécessaire.  Les sections suivantes montrent comment les fichiers de variables définis par l'utilisateur sont configurés.

====


=== Configuration des fichiers de paramètres

[%collapsible%open]
====
. Cible Ansible `hosts` configuration du fichier :
+
[source, shell]
----
# Enter NetApp ASA controller management IP address
[ontap]
172.16.9.32

# Enter Oracle servers names to be deployed one by one, follow by each Oracle server public IP address, and ssh private key of admin user for the server.
[oracle]
ora_01 ansible_host=10.61.180.21 ansible_ssh_private_key_file=ora_01.pem
ora_02 ansible_host=10.61.180.23 ansible_ssh_private_key_file=ora_02.pem

----


. Mondial `vars/vars.yml` configuration du fichier
+
[source, shell]
----
#############################################################################################################
######                 Oracle 19c deployment global user configurable variables                        ######
######                 Consolidate all variables from ONTAP, linux and oracle                          ######
#############################################################################################################

#############################################################################################################
######                 ONTAP env specific config variables                                             ######
#############################################################################################################

# Enter the supported ONTAP platform: on-prem, aws-fsx.
ontap_platform: on-prem

# Enter ONTAP cluster management user credentials
username: "xxxxxxxx"
password: "xxxxxxxx"


###### on-prem platform specific user defined variables ######

# Enter Oracle SVM iSCSI lif addresses. Each controller configures with dual paths iscsi_a, iscsi_b for redundancy
ora_iscsi_lif_mgmt:
  - {name: '{{ svm_name }}_mgmt', address: 172.21.253.220, netmask: 255.255.255.0, vlan_name: ora_mgmt, vlan_id: 3509}

ora_iscsi_lifs_node1:
  - {name: '{{ svm_name }}_lif_1a', address: 172.21.234.221, netmask: 255.255.255.0, vlan_name: ora_iscsi_a, vlan_id: 3490}
  - {name: '{{ svm_name }}_lif_1b', address: 172.21.235.221, netmask: 255.255.255.0, vlan_name: ora_iscsi_b, vlan_id: 3491}
ora_iscsi_lifs_node2:
  - {name: '{{ svm_name }}_lif_2a', address: 172.21.234.223, netmask: 255.255.255.0, vlan_name: ora_iscsi_a, vlan_id: 3490}
  - {name: '{{ svm_name }}_lif_2b', address: 172.21.235.223, netmask: 255.255.255.0, vlan_name: ora_iscsi_b, vlan_id: 3491}


#############################################################################################################
###                   Linux env specific config variables                                                 ###
#############################################################################################################

# Enter RHEL subscription to enable repo
redhat_sub_username: xxxxxxxx
redhat_sub_password: "xxxxxxxx"


#############################################################################################################
###                   Oracle DB env specific config variables                                             ###
#############################################################################################################

# Enter Database domain name
db_domain: solutions.netapp.com

# Enter initial password for all required Oracle passwords. Change them after installation.
initial_pwd_all: xxxxxxxx

----


. Serveur de base de données local `host_vars/host_name.yml` configuration
+
[source, shell]
----
# User configurable Oracle host specific parameters

# Enter container database SID. By default, a container DB is created with 3 PDBs within the CDB
oracle_sid: NTAP1

# Enter database shared memory size or SGA. CDB is created with SGA at 75% of memory_limit, MB. The grand total of SGA should not exceed 75% available RAM on node.
memory_limit: 8192

----


====


=== Exécution du manuel de jeu

[%collapsible%open]
====
Il existe au total six playbooks dans la boîte à outils d’automatisation.  Chacun exécute des blocs de tâches différents et sert des objectifs différents.

....
0-all_playbook.yml - execute playbooks from 1-4 in one playbook run.
1-ansible_requirements.yml - set up Ansible controller with required libs and collections.
2-linux_config.yml - execute Linux kernel configuration on Oracle DB servers.
3-ontap_config.yml - configure ONTAP svm/volumes/luns for Oracle database and grant DB server access to luns.
4-oracle_config.yml - install and configure Oracle on DB servers for grid infrastructure and create a container database.
5-destroy.yml - optional to undo the environment to dismantle all.
....
Il existe trois options pour exécuter les playbooks avec les commandes suivantes.

. Exécutez tous les playbooks de déploiement en une seule exécution combinée.
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml
----
. Exécutez les playbooks un par un avec la séquence de nombres de 1 à 4.
+
[source, cli]]
----
ansible-playbook -i hosts 1-ansible_requirements.yml -u admin -e @vars/vars.yml
----
+
[source, cli]
----
ansible-playbook -i hosts 2-linux_config.yml -u admin -e @vars/vars.yml
----
+
[source, cli]
----
ansible-playbook -i hosts 3-ontap_config.yml -u admin -e @vars/vars.yml
----
+
[source, cli]
----
ansible-playbook -i hosts 4-oracle_config.yml -u admin -e @vars/vars.yml
----
. Exécutez 0-all_playbook.yml avec une balise.
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t ansible_requirements
----
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t linux_config
----
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t ontap_config
----
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t oracle_config
----
. Défaire l'environnement
+
[source, cli]
----
ansible-playbook -i hosts 5-destroy.yml -u admin -e @vars/vars.yml
----


====


=== Validation post-exécution

[%collapsible%open]
====
Après l'exécution du playbook, connectez-vous au serveur Oracle DB en tant qu'utilisateur Oracle pour valider que l'infrastructure de grille et la base de données Oracle ont été créées avec succès.  Voici un exemple de validation de base de données Oracle sur l’hôte ora_01.

. Valider l’infrastructure du réseau et les ressources créées.
+
....

[oracle@ora_01 ~]$ df -h
Filesystem                    Size  Used Avail Use% Mounted on
devtmpfs                      7.7G   40K  7.7G   1% /dev
tmpfs                         7.8G  1.1G  6.7G  15% /dev/shm
tmpfs                         7.8G  312M  7.5G   4% /run
tmpfs                         7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/rhel-root          44G   38G  6.8G  85% /
/dev/sda1                    1014M  258M  757M  26% /boot
tmpfs                         1.6G   12K  1.6G   1% /run/user/42
tmpfs                         1.6G  4.0K  1.6G   1% /run/user/1000
/dev/mapper/ora_01_biny_01p1   40G   21G   20G  52% /u01
[oracle@ora_01 ~]$ asm
[oracle@ora_01 ~]$ crsctl stat res -t
--------------------------------------------------------------------------------
Name           Target  State        Server                   State details
--------------------------------------------------------------------------------
Local Resources
--------------------------------------------------------------------------------
ora.DATA.dg
               ONLINE  ONLINE       ora_01                   STABLE
ora.LISTENER.lsnr
               ONLINE  INTERMEDIATE ora_01                   Not All Endpoints Re
                                                             gistered,STABLE
ora.LOGS.dg
               ONLINE  ONLINE       ora_01                   STABLE
ora.asm
               ONLINE  ONLINE       ora_01                   Started,STABLE
ora.ons
               OFFLINE OFFLINE      ora_01                   STABLE
--------------------------------------------------------------------------------
Cluster Resources
--------------------------------------------------------------------------------
ora.cssd
      1        ONLINE  ONLINE       ora_01                   STABLE
ora.diskmon
      1        OFFLINE OFFLINE                               STABLE
ora.driver.afd
      1        ONLINE  ONLINE       ora_01                   STABLE
ora.evmd
      1        ONLINE  ONLINE       ora_01                   STABLE
ora.ntap1.db
      1        ONLINE  ONLINE       ora_01                   Open,HOME=/u01/app/o
                                                             racle/product/19.0.0
                                                             /NTAP1,STABLE
--------------------------------------------------------------------------------
[oracle@ora_01 ~]$

....
+

NOTE: Ignorer le `Not All Endpoints Registered` dans les détails de l'État.  Cela résulte d'un conflit entre l'enregistrement manuel et dynamique de la base de données avec l'écouteur et peut être ignoré en toute sécurité.

. Vérifiez que le pilote de filtre ASM fonctionne comme prévu.
+
....

[oracle@ora_01 ~]$ asmcmd
ASMCMD> lsdg
State    Type    Rebal  Sector  Logical_Sector  Block       AU  Total_MB  Free_MB  Req_mir_free_MB  Usable_file_MB  Offline_disks  Voting_files  Name
MOUNTED  EXTERN  N         512             512   4096  4194304    327680   318644                0          318644              0             N  DATA/
MOUNTED  EXTERN  N         512             512   4096  4194304     81920    78880                0           78880              0             N  LOGS/
ASMCMD> lsdsk
Path
AFD:ORA_01_DAT1_01
AFD:ORA_01_DAT1_03
AFD:ORA_01_DAT1_05
AFD:ORA_01_DAT1_07
AFD:ORA_01_DAT2_02
AFD:ORA_01_DAT2_04
AFD:ORA_01_DAT2_06
AFD:ORA_01_DAT2_08
AFD:ORA_01_LOGS_01
AFD:ORA_01_LOGS_02
ASMCMD> afd_state
ASMCMD-9526: The AFD state is 'LOADED' and filtering is 'ENABLED' on host 'ora_01'
ASMCMD>

....
. Connectez-vous à Oracle Enterprise Manager Express pour valider la base de données.
+
image:automation-ora-asa-em-001.png["Cette image fournit l'écran de connexion pour Oracle Enterprise Manager Express"] image:automation-ora-asa-em-002.png["Cette image fournit une vue de la base de données du conteneur à partir d'Oracle Enterprise Manager Express"]

+
....
Enable additional port from sqlplus for login to individual container database or PDBs.

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 NTAP1_PDB1                     READ WRITE NO
         4 NTAP1_PDB2                     READ WRITE NO
         5 NTAP1_PDB3                     READ WRITE NO
SQL> alter session set container=NTAP1_PDB1;

Session altered.

SQL> select dbms_xdb_config.gethttpsport() from dual;

DBMS_XDB_CONFIG.GETHTTPSPORT()
------------------------------
                             0

SQL> exec DBMS_XDB_CONFIG.SETHTTPSPORT(5501);

PL/SQL procedure successfully completed.

SQL> select dbms_xdb_config.gethttpsport() from dual;

DBMS_XDB_CONFIG.GETHTTPSPORT()
------------------------------
                          5501

login to NTAP1_PDB1 from port 5501.
....
+
image:automation-ora-asa-em-003.png["Cette image fournit une vue de la base de données PDB à partir d'Oracle Enterprise Manager Express"]



====


=== Sauvegarde, restauration et clonage Oracle avec SnapCenter

[%collapsible%open]
====
Se référer à TR-4979link:../oracle/aws-ora-fsx-vmc-guestmount.html#oracle-backup-restore-and-clone-with-snapcenter["Oracle simplifié et autogéré dans VMware Cloud sur AWS avec FSx ONTAP monté en invité"^] section `Oracle backup, restore, and clone with SnapCenter` pour plus de détails sur la configuration de SnapCenter et l'exécution des flux de travail de sauvegarde, de restauration et de clonage de la base de données.

====


== Où trouver des informations supplémentaires

Pour en savoir plus sur les informations décrites dans ce document, consultez les documents et/ou sites Web suivants :

* NETAPP ASA: baie SAN entièrement flash
+
link:https://www.netapp.com/data-storage/all-flash-san-storage-array/["https://www.netapp.com/data-storage/all-flash-san-storage-array/"^]

* Installation d'Oracle Grid Infrastructure pour un serveur autonome avec une nouvelle installation de base de données
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-oracle-grid-infrastructure-for-a-standalone-server-with-a-new-database-installation.html#GUID-0B1CEE8C-C893-46AA-8A6A-7B5FAAEC72B3["https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-oracle-grid-infrastructure-for-a-standalone-server-with-a-new-database-installation.html#GUID-0B1CEE8C-C893-46AA-8A6A-7B5FAAEC72B3"^]

* Installation et configuration de la base de données Oracle à l'aide de fichiers de réponses
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-and-configuring-oracle-database-using-response-files.html#GUID-D53355E9-E901-4224-9A2A-B882070EDDF7["https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-and-configuring-oracle-database-using-response-files.html#GUID-D53355E9-E901-4224-9A2A-B882070EDDF7"^]

* Utiliser Red Hat Enterprise Linux 8.2 avec ONTAP
+
link:https://docs.netapp.com/us-en/ontap-sanhost/hu_rhel_82.html#all-san-array-configurations["https://docs.netapp.com/us-en/ontap-sanhost/hu_rhel_82.html#all-san-array-configurations"^]


